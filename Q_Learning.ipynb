{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrJsPoqtJBQC2Pj/pgWVgk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamayodb/CCRNFLRL/blob/main/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning"
      ],
      "metadata": {
        "id": "efZkT8apzg0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_miHU8UCjgEw",
        "outputId": "051ee4a0-c5a8-497e-f867-42c3c3fcf180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "Training done. Last-50 avg reward: 4.60\n",
            "\n",
            "Greedy run after training:\n",
            "\n",
            "Step 0: (action=R)\n",
            "> . * #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 1: (action=R)\n",
            "S > * #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 2: (action=D)\n",
            "S . v #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 3: (action=L)\n",
            "S . . #\n",
            ". . < .\n",
            "# * . .\n",
            "\n",
            "Step 4: (action=D)\n",
            "S . . #\n",
            ". v . .\n",
            "# * . .\n",
            "\n",
            "--- Summary ---\n",
            "Finished: True\n",
            "Steps: 5\n",
            "Total reward: 5\n",
            "\n",
            "Q at START: {'U': 3.08, 'D': 1.93, 'L': 3.08, 'R': 4.3}\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# =============== Grid ===============\n",
        "# S = start, . = free. # =  wall, * = dirt, (clean once)\n",
        "\n",
        "GRID = [\n",
        "    ['S', '.', '*', '#'],\n",
        "    ['.', '.', '.', '.'],\n",
        "    ['#', '*', '.', '.'],\n",
        "]\n",
        "ROWS, COLS = len(GRID), len(GRID[0])\n",
        "ACTIONS = ['U', 'D', 'L', 'R']\n",
        "ARROW = {'U': '^', 'D': 'v', 'L': '<', 'R': '>'}\n",
        "\n",
        "# Rewards\n",
        "STEP_REWARD = -1\n",
        "CLEAN_REWARD = +5\n",
        "\n",
        "# Q-Learning params (tweak for class demos)\n",
        "SEED = 7\n",
        "EPISODES = 3000\n",
        "ALPHA = 0.10\n",
        "GAMMA = 0.95\n",
        "EPS_START  = 0.30\n",
        "EPS_END = 0.05\n",
        "MAX_STEPS = 80 # per training episode\n",
        "DEMO_STEPS = 60 # cap for greedy demo\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "# =============== Locate Start and Dirt ===============\n",
        "\n",
        "START = None\n",
        "DIRT_LIST = []\n",
        "for r in range (ROWS):\n",
        "  for c in range (COLS):\n",
        "    if GRID[r][c] == 'S':\n",
        "      START = (r, c)\n",
        "    elif GRID[r][c] == '*':\n",
        "      DIRT_LIST.append((r, c))\n",
        "DIRT_INDEX = {pos: i for i, pos in enumerate(DIRT_LIST)}\n",
        "ALL_CLEAN_MASK = (1 << len(DIRT_LIST)) - 1\n",
        "\n",
        "def in_bounds(r,c): return 0 <= r < ROWS and 0 <= c < COLS\n",
        "def is_wall(r,c): return GRID [r][c] == '#'\n",
        "\n",
        "# =============== Environment ===============\n",
        "\n",
        "def step_env(state, action):\n",
        "  \"\"\"\n",
        "  State = (row, col, cleaned_mask)\n",
        "  Move: apply STEP_REWARD; if first time on a '*' add CLEAN_REWARD once.\n",
        "  Returns: (new_state, reward, done)\n",
        "  \"\"\"\n",
        "  r, c, cleaned_mask = state\n",
        "  nr, nc = r, c\n",
        "  if action == 'U': nr -= 1\n",
        "  elif action == 'D': nr += 1\n",
        "  elif action == 'L': nc -= 1\n",
        "  elif action == 'R': nc += 1\n",
        "\n",
        "  # Blocked -> stay\n",
        "  if not in_bounds(nr, nc) or is_wall(nr, nc):\n",
        "    nr, nc = r, c\n",
        "\n",
        "  reward = STEP_REWARD\n",
        "  mask = cleaned_mask # Initialize mask from state\n",
        "  if (nr, nc) in DIRT_INDEX:\n",
        "    bit = 1 << DIRT_INDEX[(nr, nc)]\n",
        "    if (mask & bit) == 0:\n",
        "      mask |= bit\n",
        "      reward += CLEAN_REWARD\n",
        "\n",
        "  done = (mask == ALL_CLEAN_MASK)\n",
        "  return (nr, nc, mask), reward, done\n",
        "\n",
        "# =============== Q-Learning Helpers ===============\n",
        "\n",
        "def agrmax_q(Q, s):\n",
        "  best_v = float('-inf')\n",
        "  ties = []\n",
        "  for a in ACTIONS:\n",
        "    v = Q.get((s,a), 0.0)\n",
        "    if v > best_v:\n",
        "      best_v, ties= v, [a]\n",
        "    elif abs(v - best_v) < 1e-12:\n",
        "      ties.append(a)\n",
        "  return random.choice(ties)\n",
        "\n",
        "def epsilon_greedy(Q, s, eps):\n",
        "  if random.random() < eps:\n",
        "    return random.choice(ACTIONS), True\n",
        "  return agrmax_q(Q, s), False\n",
        "\n",
        "# =============== Train (Q-Table) ===============\n",
        "\n",
        "def train_q_learning(episodes=EPISODES):\n",
        "  Q = {}\n",
        "  rewards = []\n",
        "\n",
        "  def eps_schedule(ep):\n",
        "    # Linear decay from EPS_START to EPS_END\n",
        "    t = ep / max(1, episodes - 1)\n",
        "    return EPS_START * (1 - t) + EPS_END * t\n",
        "\n",
        "  for ep in range(episodes):\n",
        "    s = (START[0], START[1], 0) # start with nothing cleaned\n",
        "    total, steps, done = 0, 0, False\n",
        "    eps = eps_schedule(ep)\n",
        "\n",
        "    while not done and steps < MAX_STEPS:\n",
        "      a, _ = epsilon_greedy(Q, s, eps)\n",
        "      s2, r, done = step_env(s, a)\n",
        "      total += r\n",
        "\n",
        "      # Q(s,a)←Q(s,a)+α[r+γa′max​Q(s′,a′)−Q(s,a)]\n",
        "      max_next = max([Q.get((s2, ap), 0.0) for ap in ACTIONS])\n",
        "      td_target = r + (0 if done else GAMMA * max_next)\n",
        "      old = Q.get((s,a), 0.0)\n",
        "      Q[(s,a)] = old + ALPHA * (td_target - old)\n",
        "\n",
        "      s = s2\n",
        "      steps += 1\n",
        "\n",
        "    rewards.append(total)\n",
        "  return Q, rewards\n",
        "\n",
        "# =============== Greedy Demo (Use Learned Q) ===============\n",
        "\n",
        "def greedy_run(Q, max_steps=DEMO_STEPS):\n",
        "  s = (START[0], START[1], 0)\n",
        "  path = [s]\n",
        "  actions = []\n",
        "  total = 0\n",
        "  for _ in range(max_steps):\n",
        "    a = agrmax_q(Q, s)\n",
        "    s, r, done = step_env(s, a)\n",
        "    total += r\n",
        "    path.append(s)\n",
        "    actions.append(a)\n",
        "    if done: break\n",
        "  return path, actions, total, done\n",
        "\n",
        "# =============== Pretty Print ===============\n",
        "\n",
        "def show_grid(state, next_a=None):\n",
        "  r0, c0, mask = state\n",
        "  lines = []\n",
        "  for r in range(ROWS):\n",
        "    row = []\n",
        "    for c in range(COLS):\n",
        "      if (r,c) == (r0, c0):\n",
        "        row.append('A' if next_a is None else ARROW[next_a])\n",
        "      elif GRID[r][c] == '#':\n",
        "        row.append('#')\n",
        "      elif GRID[r][c] == '*':\n",
        "        bit = 1 << DIRT_INDEX[(r,c)]\n",
        "        row.append('*' if (mask & bit) == 0 else '.')\n",
        "      else:\n",
        "        row.append(GRID[r][c])\n",
        "    lines.append(\" \".join(row))\n",
        "  print('\\n'.join(lines))\n",
        "\n",
        "# =============== Run ===============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Training...\")\n",
        "    Q, rewards = train_q_learning()\n",
        "    last50 = sum(rewards[-50:]) / max(1, len(rewards[-50:]))\n",
        "    print(f\"Training done. Last-50 avg reward: {last50:.2f}\")\n",
        "\n",
        "    path, actions, total, done = greedy_run(Q)\n",
        "    print(\"\\nGreedy run after training:\")\n",
        "    for i, s in enumerate(path[:-1]):\n",
        "        a = actions[i] if i < len(actions) else None\n",
        "        print(f\"\\nStep {i}: (action={a})\")\n",
        "        show_grid(s, next_a=a)\n",
        "\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(\"Finished:\", done)\n",
        "    print(\"Steps:\", len(actions))\n",
        "    print(\"Total reward:\", total)\n",
        "\n",
        "# Optional: peek at Q-values for START\n",
        "def q_row(Qtab, state):\n",
        "    return {a: round(Qtab.get((state, a), 0.0), 2) for a in ACTIONS}\n",
        "\n",
        "print(\"\\nQ at START:\", q_row(Q, (START[0], START[1], 0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment Trials"
      ],
      "metadata": {
        "id": "1LzzJNlGzbQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= Experiment Trials =================\n",
        "\n",
        "TRIALS = {\n",
        "    \"A\": {\"EPS_START\": 1.0, \"EPS_END\": 0.05, \"ALPHA\": 0.10},\n",
        "    \"B\": {\"EPS_START\": 1.0, \"EPS_END\": 0.05, \"ALPHA\": 0.15},\n",
        "    \"C\": {\"EPS_START\": 0.30, \"EPS_END\": 0.05, \"ALPHA\": 0.10},\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = []\n",
        "\n",
        "    for name, params in TRIALS.items():\n",
        "        print(f\"\\n===== Trial {name} =====\")\n",
        "        EPS_START = params[\"EPS_START\"]\n",
        "        EPS_END   = params[\"EPS_END\"]\n",
        "        ALPHA     = params[\"ALPHA\"]\n",
        "\n",
        "        # Train\n",
        "        Q, rewards = train_q_learning()\n",
        "        last50 = sum(rewards[-50:]) / max(1, len(rewards[-50:]))\n",
        "\n",
        "        # Greedy run\n",
        "        path, actions, total, done = greedy_run(Q)\n",
        "\n",
        "        print(\"\\nGreedy run after training:\")\n",
        "        for i, s in enumerate(path[:-1]):\n",
        "            a = actions[i] if i < len(actions) else None\n",
        "            print(f\"\\nStep {i}: (action={a})\")\n",
        "            show_grid(s, next_a=a)\n",
        "\n",
        "        results.append({\n",
        "            \"Trial\": name,\n",
        "            \"Q at START\": q_row(Q, (START[0], START[1], 0)),\n",
        "            \"Last-50 Avg Reward\": round(last50, 2),\n",
        "            \"Finished?\": done,\n",
        "            \"Steps\": len(actions),\n",
        "            \"Total Reward\": total\n",
        "        })\n",
        "\n",
        "    # Summary table\n",
        "    print(\"\\n================ Summary Table ================\")\n",
        "    print(f\"{'Trial':<5} | {'Last-50 Avg Reward':<20} | {'Finished?':<10} | {'Steps':<6} | {'Total Reward':<12} | Q at START\")\n",
        "    print(\"-\"*90)\n",
        "    for row in results:\n",
        "        print(f\"{row['Trial']:<5} | {row['Last-50 Avg Reward']:<20} | {str(row['Finished?']):<10} | {row['Steps']:<6} | {row['Total Reward']:<12} | {row['Q at START']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xniLkcXoyb6n",
        "outputId": "520931bc-9e5c-4891-a653-060c889996e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial A =====\n",
            "\n",
            "Greedy run after training:\n",
            "\n",
            "Step 0: (action=R)\n",
            "> . * #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 1: (action=R)\n",
            "S > * #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 2: (action=D)\n",
            "S . v #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 3: (action=L)\n",
            "S . . #\n",
            ". . < .\n",
            "# * . .\n",
            "\n",
            "Step 4: (action=D)\n",
            "S . . #\n",
            ". v . .\n",
            "# * . .\n",
            "\n",
            "===== Trial B =====\n",
            "\n",
            "Greedy run after training:\n",
            "\n",
            "Step 0: (action=R)\n",
            "> . * #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 1: (action=R)\n",
            "S > * #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 2: (action=D)\n",
            "S . v #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 3: (action=L)\n",
            "S . . #\n",
            ". . < .\n",
            "# * . .\n",
            "\n",
            "Step 4: (action=D)\n",
            "S . . #\n",
            ". v . .\n",
            "# * . .\n",
            "\n",
            "===== Trial C =====\n",
            "\n",
            "Greedy run after training:\n",
            "\n",
            "Step 0: (action=R)\n",
            "> . * #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 1: (action=R)\n",
            "S > * #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 2: (action=L)\n",
            "S . < #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 3: (action=D)\n",
            "S v . #\n",
            ". . . .\n",
            "# * . .\n",
            "\n",
            "Step 4: (action=D)\n",
            "S . . #\n",
            ". v . .\n",
            "# * . .\n",
            "\n",
            "================ Summary Table ================\n",
            "Trial | Last-50 Avg Reward   | Finished?  | Steps  | Total Reward | Q at START\n",
            "------------------------------------------------------------------------------------------\n",
            "A     | 4.6                  | True       | 5      | 5            | {'U': 3.08, 'D': 3.08, 'L': 3.08, 'R': 4.3}\n",
            "B     | 4.84                 | True       | 5      | 5            | {'U': 3.08, 'D': 3.08, 'L': 3.08, 'R': 4.3}\n",
            "C     | 4.78                 | True       | 5      | 5            | {'U': 3.08, 'D': 1.93, 'L': 3.08, 'R': 4.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How ε (exploration) and α (learning rate) change learning"
      ],
      "metadata": {
        "id": "K7y7dqQYzB1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which trial has the highest Last-50 avg reward? Cite the number.\n",
        "- Trial B, with 4.84\n",
        "---\n",
        "\n",
        "2. From START, what is the first greedy move in A, B, and C? Use the largest Q to justify.\n",
        "- In all three trials, the first greedy move is Right (R) because Q(R) = 4.3, which is the highest among the actions.\n",
        "---\n",
        "\n",
        "3. In Trial C, Down = 1.93 is much lower. What does that imply, and what one change to ε or α would you try to improve learning?\n",
        "- It implies the agent learned that Down is not a good option. I’d try using a larger ε to encourage more exploration or increasing α to strengthen updates.\n",
        "---\n",
        "\n",
        "4. If you raise α from 0.10 → 0.20 (same episodes), what happens to the speed of learning and the stability of the Last-50 curve? Explain briefly.\n",
        "- Raising α to 0.20 makes learning faster but less stable.\n",
        "---\n",
        "\n",
        "5. Two settings reach the same greedy total reward (+5), but one has a higher Last-50 avg. Which configuration (episodes/α) would you prefer in practice, and why?\n",
        "- I’d prefer the one with the higher Last-50 avg reward because it shows more consistent performance.\n"
      ],
      "metadata": {
        "id": "I5oXvxx1zvrj"
      }
    }
  ]
}